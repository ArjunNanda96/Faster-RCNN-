# -*- coding: utf-8 -*-
"""Sahachar_prefinal of cis680_4_partb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sVflEqqrz2qmyEc3UXp2DjN5iC7rTDWH
"""

from google.colab import drive
drive.mount('/content/drive')
# drive.mount("/content/drive", force_remount=True)

!git clone https://github.com/beccadsouza/CIS-680-Projects.git

import torch
import torch.nn.functional as F
from torch import nn
import numpy as np
# import utils
import torchvision
from torchvision import transforms
# from pretrained_models import pretrained_models_680
# from dataset import BuildDataset, BuildDataLoader
import time
from torchvision.models.detection.image_list import ImageList
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
from sklearn.metrics import auc
import h5py
from torch.utils.data import Dataset, DataLoader
import h5py

# import torchvision
# import torch
# from dataset import *


def pretrained_models_680(checkpoint_file,eval=True):
    import torchvision
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)

    if(eval):
        model.eval()

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    model.to(device)

    backbone = model.backbone
    rpn = model.rpn

    if(eval):
        backbone.eval()
        rpn.eval()

    rpn.nms_thresh=0.6
    checkpoint = torch.load(checkpoint_file)

    backbone.load_state_dict(checkpoint['backbone'])
    rpn.load_state_dict(checkpoint['rpn'])

    return backbone, rpn

# if __name__ == '__main__':

#     # Put the path were you save the given pretrained model
#     pretrained_path='checkpoint680.pth'
#     device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
#     backbone, rpn = pretrained_models_680(pretrained_path)

#     # we will need the ImageList from torchvision
#     from torchvision.models.detection.image_list import ImageList


#     imgs_path = './data/hw3_mycocodata_img_comp_zlib.h5'
#     masks_path = './data/hw3_mycocodata_mask_comp_zlib.h5'
#     labels_path = "./data/hw3_mycocodata_labels_comp_zlib.npy"
#     bboxes_path = "./data/hw3_mycocodata_bboxes_comp_zlib.npy"

#     paths = [imgs_path, masks_path, labels_path, bboxes_path]
#     # load the data into data.Dataset
#     dataset = BuildDataset(paths)

#     # Standard Dataloaders Initialization
#     full_size = len(dataset)
#     train_size = int(full_size * 0.8)
#     test_size = full_size - train_size

#     torch.random.manual_seed(1)
#     train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

#     batch_size = 10
#     print("batch size:", batch_size)
#     test_build_loader = BuildDataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
#     test_loader = test_build_loader.loader()


#     # Here we keep the top 20, but during training you should keep around 200 boxes from the 1000 proposals
#     keep_topK=20

#     with torch.no_grad():
#         for iter, batch in enumerate(test_loader, 0):
#             images = batch['images'].to(device)

#             # Take the features from the backbone
#             backout = backbone(images)

#             # The RPN implementation takes as first argument the following image list
#             im_lis = ImageList(images, [(800, 1088)]*images.shape[0])
#             # Then we pass the image list and the backbone output through the rpn
#             rpnout = rpn(im_lis, backout)

#             #The final output is
#             # A list of proposal tensors: list:len(bz){(keep_topK,4)}
#             proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]
#             # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}
#             fpn_feat_list= list(backout.values())

#             print("For the proposals We have a list containing "+str(len(proposals))+" tensors")
#             print("Each one with shape "+str(proposals[0].shape))
#             print("")
#             print("For the features we have a list of features for each FPN level with shapes")
#             for feat in fpn_feat_list:
#                 print(feat.shape)


#             # Visualization of the proposals
#             for i in range(batch_size):
#                 img_squeeze = transforms.functional.normalize(images[i,:,:,:].to('cpu'),
#                                                               [-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],
#                                                               [1 / 0.229, 1 / 0.224, 1 / 0.225], inplace=False)
#                 fig,ax=plt.subplots(1,1)
#                 ax.imshow(img_squeeze.permute(1,2,0))


#                 for box in proposals[i]:
#                     box=box.view(-1)
#                     rect=patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],fill=False,color='b')
#                     ax.add_patch(rect)
#                 plt.show()

#             break

import torch
from torchvision import transforms
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import h5py
import numpy as np
# from utils import *
import matplotlib.pyplot as plt
# from rpn import *
import matplotlib.patches as patches


class BuildDataset(torch.utils.data.Dataset):
    def __init__(self, path):
        #############################################
        # TODO Initialize  Dataset
        #############################################
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        self.img    = h5py.File(path[0],'r')['data']               #Array:           Shape: (len(dataset),3,300,400)
        self.mask   = h5py.File(path[1],'r')['data']               #Array:           Shape: (total_masks_in dataset, 300, 400)
        self.labels = np.load(path[2], allow_pickle=True)          #Array of arrays: Shape: (len(dataset))  Each array: Shape: (num_labels for that image)
        self.bbox   = np.load(path[3], allow_pickle=True)          #Array of arrays: Shape: (len(dataset))  Each array: Shape: (num_labels for that image, 4)
        #Transforms
        self.normalize = transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))

        self.aligned_masks = [] #List of mask tensors    i.e.  len(dataset) (num_masks_for_an_image, 300,400)

        i = 0
        for l in range(self.labels.shape[0]):
            num_labels = self.labels[l].size
            # if l ==0:
            #     self.aligned_masks.append([])
            #     continue
            mask_clubbed = np.zeros((num_labels,300,400))#,dtype = torch.int)
            for mask_idx in range(num_labels):
                mask_clubbed[mask_idx,:,:] = self.mask[i,:,:]
                i+=1
            self.aligned_masks.append(mask_clubbed)

    # In this function for given index we rescale the image and the corresponding  masks, boxes
    # and we return them as output
    # output:
        # transed_img
        # label
        # transed_mask
        # transed_bbox
        # index
    def __getitem__(self, index):
        ################################
        # TODO return transformed images,labels,masks,boxes,index
        ################################
        img   = self.img[index,:,:,:]                             #Array: Shape: (3,300,400)
        mask  = self.aligned_masks[index]                         #Array: Shape: (num_masks_for_an_image, 300,400)  
        label = self.labels[index]                                #Array: Shape: (num_labels)                       
        bbox  = self.bbox[index]                                  #Array: Shape: (num_labels,4)
        
        img   = torch.tensor(img.astype(np.float),  dtype = torch.float).to(self.device)
        mask  = torch.tensor(mask.astype(np.float), dtype = torch.float).to(self.device)
        label = torch.tensor(label,                 dtype = torch.float).to(self.device)
        bbox  = torch.tensor(bbox.astype(np.float), dtype = torch.float).to(self.device)


        transed_img, transed_mask, transed_bbox = self.pre_process_batch(img, mask, bbox)

        assert transed_img.shape == (3,800,1088)
        assert transed_bbox.shape[0] == transed_mask.shape[0]

        
        return transed_img, label, transed_mask, transed_bbox, index



    # This function preprocess the given image, mask, box by rescaling them appropriately
    #Input: 
    #        img:  (3,300,400)
    #        mask: (n_box,300,400)
    #        bbox: (n_box,4)
    # output:
    #        img: (3,800,1088)
    #        mask: (n_box,800,1088)
    #        box: (n_box,4)
    def pre_process_batch(self, img, mask, bbox):
        #######################################
        # TODO apply the correct transformation to the images,masks,boxes
        ######################################

        scale_factor_x = 800/300
        scale_factor_y = 1066/400

        img =  img/255.0 #Normalization
        img =  torch.unsqueeze(img, 0)                                                 #(3,300,400)   -> (1,3,300,400)
        img = torch.nn.functional.interpolate(img, size=(800, 1066), mode='bilinear')  #(1,3,300,400) -> (1,3,800,1066)
        img =  self.normalize(img[0])                                                  #(1,3,800,1066)-> (3,800,1066)
        img =  torch.nn.functional.pad(img, pad=(11,11), mode='constant',value=0)      #(3,800,1066)  -> (3,800,1088)
        img = img.squeeze(0)                                                           #(3,800,1066)  -> (1,3,800,1088)

        mask = mask.unsqueeze(0)                                                                     #(n_box,300,400)    -> (1,n_box,300,400)
        mask = torch.nn.functional.interpolate(mask, size = (800,1066), mode = 'bilinear')           #(1,n_box,300,400)  -> (1,n_box,800,1066)
        mask = torch.nn.functional.pad(mask, pad=(11,11), mode='constant',value=0)                   #(1,n_box,800,1066) -> (1,n_box,800,1088)
        

        bbox[:,0] = bbox[:,0] * scale_factor_x
        bbox[:,2] = bbox[:,2] * scale_factor_x
        bbox[:,1] = bbox[:,1] * scale_factor_y
        bbox[:,3] = bbox[:,3] * scale_factor_y
        bbox[:,0] += 11 
        bbox[:,2] += 11 #Accounting for changes in x due to padding            

        assert img.squeeze(0).shape == (3, 800, 1088)
        assert bbox.shape[0] == mask.squeeze(0).shape[0]
        return img.squeeze(0), mask.squeeze(0), bbox

    
    def __len__(self):
        # return len(self.imgs_data)
        return self.img.shape[0]




class BuildDataLoader(torch.utils.data.DataLoader):
    def __init__(self, dataset, batch_size, shuffle, num_workers):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers


    # output:
    #  dict{images: (bz, 3, 800, 1088)
    #       labels: list:len(bz)
    #       masks: list:len(bz){(n_obj, 800,1088)}
    #       bbox: list:len(bz){(n_obj, 4)}
    #       index: list:len(bz)
    def collect_fn(self, batch):
        
        out_batch = {}
        # bz = len(batch)
        
        img_list = []
        label_list = []
        mask_list = []
        bbox_list = []
        index_list = []
        
        for img, label, mask, bbox, index in batch:
          img_list.append(img)
          label_list.append(label)
          mask_list.append(mask)
          bbox_list.append(bbox)
          index_list.append(index)  

        out_batch['images'] = torch.stack(img_list,dim=0)
        out_batch['labels'] = label_list
        out_batch['masks']  = mask_list
        out_batch['bbox']   = bbox_list
        out_batch['index']  = index_list
        return out_batch


    def loader(self):
        return DataLoader(self.dataset,
                          batch_size=self.batch_size,
                          shuffle=self.shuffle,
                          num_workers=self.num_workers,
                          collate_fn=self.collect_fn)

if __name__ == '__main__':
    # file path and make a list
    imgs_path = '/content/drive/MyDrive/CIS680_2021/HW3/hw3_mycocodata_img_comp_zlib.h5'
    masks_path = '/content/drive/MyDrive/CIS680_2021/HW3/hw3_mycocodata_mask_comp_zlib.h5'
    labels_path = '/content/drive/MyDrive/CIS680_2021/HW3/hw3_mycocodata_labels_comp_zlib.npy'
    bboxes_path = '/content/drive/MyDrive/CIS680_2021/HW3/hw3_mycocodata_bboxes_comp_zlib.npy'

    

    paths = [imgs_path, masks_path, labels_path, bboxes_path]
    # load the data into data.Dataset
    dataset = BuildDataset(paths)
    pretrained_path = '/content/drive/MyDrive/CIS680_2021/HW_4/checkpoint680.pth'

# build the dataloader
# set 20% of the dataset as the training data
full_size = len(dataset)
print(full_size)
train_size = int(full_size * 0.8)
test_size = full_size - train_size
# random split the dataset into training and testset

train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

# push the randomized training data into the dataloader

batch_size = 2
train_build_loader = BuildDataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
train_loader = train_build_loader.loader()
test_build_loader = BuildDataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)
test_loader = test_build_loader.loader()

class BoxHead(torch.nn.Module):
    def __init__(self,Classes=3,P=7, device = 'cuda' , evaluate = False):
        super(BoxHead,self).__init__()
        self.C=Classes
        self.P=P
        self.device = device
        self.init_layers()
        self._init_weights()
        self.evaluate = evaluate

    def init_weights(self, l):
        if type(l) == nn.Linear:
          nn.init.normal_(l.weight, mean=0.0, std=0.01)
          if l.bias != None:
            l.bias.data.fill_(0)

    def _init_weights(self):
        ## TODO: initialize the weights
        for l in self.intermediate:
          l.apply(self.init_weights)

        self.classifier.apply(self.init_weights)
        self.regressor.apply(self.init_weights)        
        
        
    def init_layers(self):
        self.intermediate = nn.ModuleList()
        self.intermediate.append(nn.Sequential(torch.nn.Linear(256*self.P*self.P, 1024),torch.nn.ReLU(inplace=False)))
        self.intermediate.append(nn.Sequential(torch.nn.Linear(1024, 1024),torch.nn.ReLU(inplace=False)))
        
        #self.classifier = nn.Sequential(nn.Linear(1024, self.C + 1), nn.Softmax())
        self.classifier = nn.Sequential(nn.Linear(1024, self.C + 1))
        self.regressor =  nn.Sequential(nn.Linear(1024, 4*self.C))
        

    #  This function assigns to each proposal either a ground truth box or the background class (we assume background class is 0)
    #  Input:
    #       proposals: list:len(bz){(per_image_proposals,4)} ([x1,y1,x2,y2] format)
    #       gt_labels: list:len(bz) {(n_obj)}
    #       bbox: list:len(bz){(n_obj, 4)}
    #  Output: (make sure the ordering of the proposals are consistent with MultiScaleRoiAlign)
    #       labels: (total_proposals,1) (the class that the proposal is assigned)
    #       regressor_target: (total_proposals,4) (target encoded in the [t_x,t_y,t_w,t_h] format)
    def create_ground_truth(self,proposals,gt_labels,bbox):
        labels = []
        regressor_target=[]
        
        for i in range(len(proposals)):
            iou= IOU(proposals[i],bbox[i])                      #[n_proposals, ground_truth_boxes]
            # iou= iou1(proposals[i],bbox[i])
            max_iou, gt_box_index = torch.max(iou, dim=1)             # n_proposals
            
            l = torch.zeros(proposals[i].shape[0],device = self.device)                    #n_proposals
            l[max_iou>0.5] = gt_labels[i][gt_box_index[max_iou>0.5]]     #n_proposals
            # print(max_iou[max_iou>0.4])
            box    = torch.zeros((proposals[i].shape[0],4))

            gt_box = bbox[i][gt_box_index,:]
            
            box[:,0] = ((gt_box[:,0] + gt_box[:,2])/2 - (proposals[i][:,0]+proposals[i][:,2])/2)/(proposals[i][:,2]-proposals[i][:,0])
            box[:,1] = ((gt_box[:,1] + gt_box[:,3])/2 - (proposals[i][:,1]+proposals[i][:,3])/2)/(proposals[i][:,3]-proposals[i][:,1])
            box[:,2] = torch.log((gt_box[:,2] - gt_box[:,0])/ (proposals[i][:,2] - proposals[i][:,0]))
            box[:,3] = torch.log((gt_box[:,3] - gt_box[:,1])/ (proposals[i][:,3] - proposals[i][:,1]))
            
            labels.append(l)
            regressor_target.append(box)
            
        labels = torch.hstack(labels).reshape(-1,1)                       #dim : (total_proposals,1)
        regressor_target = torch.vstack(regressor_target)                 #dim : (total_prosposals,4)
            
        return labels.to(self.device),regressor_target.to(self.device)



    # This function for each proposal finds the appropriate feature map to sample and using RoIAlign it samples
    # a (256,P,P) feature map. This feature map is then flattened into a (256*P*P) vector
    # Input:
    #      fpn_feat_list: list:len(FPN){(bz,256,H_feat,W_feat)}
    #      proposals: list:len(bz){(per_image_proposals,4)} ([x1,y1,x2,y2] format)
    #      P: scalar
    # Output:
    #      feature_vectors: (total_proposals, 256*P*P)  (make sure the ordering of the proposals are the same as the ground truth creation)
    def MultiScaleRoiAlign(self, fpn_feat_list,proposals,P=7):
        #####################################
        # Here you can use torchvision.ops.RoIAlign check the docs
        ####################################
        
        
        fpn_boxes = [[],[],[],[]]           #For each level of feature pyramid
        orig_height= 800 
        orig_width = 1088
        feature_vectors=[]
        for i in range(len(proposals)):
            for j in range(proposals[i].shape[0]):
                x1,y1,x2,y2 = proposals[i][j]
                w = x2-x1
                h = y2-y1
                k = torch.clip(torch.floor(4+torch.log2(torch.sqrt(w*h)/224)),2,5).int()
            
                stride_x = orig_width/fpn_feat_list[k-2].shape[3]
                stride_y = orig_height/fpn_feat_list[k-2].shape[2]

                box = proposals[i][j].reshape(1,-1).clone()
                box[:,0] = box[:,0] / stride_x 
                box[:,2] = box[:,2] / stride_x
                box[:,1] = box[:,1] / stride_y
                box[:,3] = box[:,3] / stride_y
                # import pdb; pdb.set_trace()   
                inp = fpn_feat_list[k-2][i].unsqueeze(0)  # dim: (1,256,H_feat,W_feat)
                op  = torchvision.ops.roi_align(inp, [box], output_size=P, 
                                      spatial_scale=1,
                                      sampling_ratio=-1)  # dim : (1,256,P,P)

                feature_vectors.append(op.view(-1))
                #fpn_boxes[k-2].append(element)
        
        feature_vectors = torch.stack(feature_vectors, dim=0) #dim = (total_proposals, 256*P*P)
        
        '''
        output = [] #Dim len(fpn_list), element: tensor: (K,C,P,P)
        for i, level in enumerate(fpn_boxes):
            level_proposals = torch.stack(level,dim=1)[0][:,1:6]   # dim: (K,5) 
            output.append(torchvision.ops.roi_align(fpn_feat_list[i], level_proposals, output_size=P, 
                                      spatial_scale=orig_height/fpn_feat_list[i].shape[2],
                                      sampling_ratio=4))
            
        '''  
            
        return feature_vectors



    # This function does the post processing for the results of the Box Head for a batch of images
    # Use the proposals to distinguish the outputs from each image
    # Input:
    #       class_logits: (total_proposals,(C+1))
    #       box_regression: (total_proposal,4*C)           ([t_x,t_y,t_w,t_h] format)
    #       proposals: list:len(bz)(per_image_proposals,4) (the proposals are produced from RPN [x1,y1,x2,y2] format)
    #       conf_thresh: scalar
    #       keep_num_preNMS: scalar (number of boxes to keep pre NMS)
    #       keep_num_postNMS: scalar (number of boxes to keep post NMS)
    # Output:
    #       boxes: list:len(bz){(post_NMS_boxes_per_image,4)}  ([x1,y1,x2,y2] format)
    #       scores: list:len(bz){(post_NMS_boxes_per_image)}   ( the score for the top class for the regressed box)
    #       labels: list:len(bz){(post_NMS_boxes_per_image)}   (top class of each regressed box)
    def postprocess_detections(self, class_logits, box_regression, proposals, conf_thresh=0.5, keep_num_preNMS=500, keep_num_postNMS=5):
        class_logits = class_logits.cpu()
        box_regression = box_regression.cpu()
      
        class_scores, class_idx = torch.max(class_logits, dim =1)

        background = class_idx==0
        class_scores[background] = 0

        class_idx_cp = class_idx.clone() - 1
        class_idx_cp[class_idx == 0] = 0

        # print("class scores", class_scores)
        # print("class scores", class_idx)

        cols_to_idx           =     np.linspace(4*class_idx_cp,4*class_idx_cp+3,4).T
        rows                  =     np.arange(box_regression.shape[0]).reshape(-1,1)

        boxes_regr        = box_regression[rows, cols_to_idx]

        boxes  = []
        scores = []
        labels = []
        iou_thresh = 0.65

        j=0
        for i in range(len(proposals)):
          proposals_per_image = proposals[i].shape[0]
          unsorted_boxes_coded = boxes_regr[j:j+proposals_per_image]
          unsorted_boxes = output_decoding(unsorted_boxes_coded,proposals[i].cpu(), device='cpu')
          
          unsorted_scores = class_scores[j:j+proposals_per_image]
          unsorted_labels = class_idx[j:j+proposals_per_image]

          #Removing out of bound boxes
          out_of_range = torch.logical_or(unsorted_boxes[:,0]<0 , unsorted_boxes[:,1]<0)
          out_of_range = torch.logical_or(out_of_range,torch.logical_or(unsorted_boxes[:,2]>1088, unsorted_boxes[:,3]>800))
          unsorted_scores[out_of_range] = 0

          sorted_scores, sorted_score_idx = torch.sort(unsorted_scores, descending = True)
          
          idx = sorted_scores>conf_thresh
          sorted_scores = sorted_scores[idx]
          sorted_score_idx = sorted_score_idx[idx]

          if (sorted_scores.shape[0]>keep_num_preNMS):
              sorted_scores = sorted_scores[:keep_num_preNMS]
              sorted_score_idx = sorted_score_idx[:keep_num_preNMS]
          
          pre_nms_scores = sorted_scores
          pre_nms_boxes = unsorted_boxes[sorted_score_idx]
          pre_nms_labels = unsorted_labels[sorted_score_idx]

          ious = IOU(pre_nms_boxes.to(self.device), pre_nms_boxes.to(self.device)).triu(diagonal = 1)
          
          #print("ious", ious)
          #print("ious shape", ious.shape)

          post_nms_idx    = (ious>iou_thresh).sum(dim=0) == 0
          #print("nms", post_nms_idx)
          post_nms_scores = pre_nms_scores[post_nms_idx]
          post_nms_boxes  = pre_nms_boxes[post_nms_idx]
          post_nms_labels = pre_nms_labels[post_nms_idx]

          if (post_nms_scores.shape[0]>keep_num_postNMS):
              boxes.append(post_nms_boxes[:keep_num_postNMS])
              scores.append(post_nms_scores[:keep_num_postNMS])
              labels.append(post_nms_labels[:keep_num_postNMS])
          else:
              boxes.append(post_nms_boxes)
              scores.append(post_nms_scores)
              labels.append(post_nms_labels)

          j= j+proposals_per_image

        # print("Len boxes", len(boxes))
        # print("Len Scores", len(scores))
        # print("Len Labels", len(labels))

        return boxes, scores, labels




    # Compute the total loss of the classifier and the regressor
    # Input:
    #      class_logits: (total_proposals,(C+1)) (as outputed from forward, not passed from softmax so we can use CrossEntropyLoss)
    #      box_preds: (total_proposals,4*C)      (as outputed from forward)
    #      labels: (total_proposals,1)
    #      regression_targets: (total_proposals,4)
    #      l: scalar (weighting of the two losses)
    #      effective_batch: scalar
    # Outpus:
    #      loss: scalar
    #      loss_class: scalar
    #      loss_regr: scalar
    def compute_loss(self,class_logits, box_preds, labels, regression_targets,l=10,effective_batch=150):
        

        M = effective_batch
        n_neg = (labels[:,0] == 0).sum().item()
        n_pos = labels.shape[0] - n_neg
        #print("Neg:",n_neg)
        #print("pos:",n_pos)
        if n_pos < (3*M/4):
          num_neg_sample       = M - n_pos
          neg_idx              = np.random.choice(n_neg, num_neg_sample, replace = False)
          pos_idx              = torch.arange(n_pos)
          
        else:
          pos_idx              = np.random.choice(n_pos, (3*M)//4, replace = False)
          neg_idx              = np.random.choice(n_neg, M - ((3*M)//4), replace = False)

        p_class_pred           =     class_logits[(labels[:,0] != 0) , :][pos_idx,:]
        n_class_pred           =     class_logits[(labels[:,0] == 0) , :][neg_idx,:]
        p_label                =     labels[ (labels[:,0] != 0) ,:][pos_idx,:]
        n_label                =     labels[ (labels[:,0] == 0) ,:][neg_idx,:]

        class_pred             =     torch.vstack((p_class_pred,n_class_pred))
        class_gt               =     torch.vstack((p_label,n_label))

        loss_cp              =     nn.CrossEntropyLoss()
        #loss_cp              =     nn.NLLLoss()
        #loss_class           =     loss_cp(torch.log(class_pred), class_gt[:,0])
        loss_class           =     loss_cp(class_pred, class_gt[:,0])
        
        p_box_pred           =     box_preds[(labels[:,0] != 0) , :][pos_idx,:]          #(no of +ve samples , 4*C)

        p_labels_to_idx      =     (p_label - 1)*4
        if p_labels_to_idx.is_cuda:
          p_labels_to_idx = p_labels_to_idx.cpu().numpy()

        col_to_idx           =     np.linspace(p_labels_to_idx,p_labels_to_idx+3,4).T
        rows                 =     np.arange(p_box_pred.shape[0]).reshape(-1,1)

        box_pred             =     p_box_pred[rows,col_to_idx]
        box_gt               =     regression_targets[(labels[:,0] != 0) , :][pos_idx,:]

        #print("box_pred", box_pred)
        #print("box_gt", box_gt)

        L1_loss              =     torch.nn.SmoothL1Loss(reduction = 'sum')
        loss_regr            =     L1_loss(box_pred.squeeze(0),box_gt) 

        loss = loss_class + l*loss_regr 
        
        
        return loss, loss_class, l*loss_regr



    # Forward the pooled feature vectors through the intermediate layer and the classifier, regressor of the box head
    # Input:
    #        feature_vectors: (total_proposals, 256*P*P)
    # Outputs:
    #        class_logits: (total_proposals,(C+1)) (we assume classes are C classes plus background, notice if you want to use
    #                                               CrossEntropyLoss you should not pass the output through softmax here)
    #        box_pred:     (total_proposals,4*C)
    def forward(self, feature_vectors):
        out = feature_vectors
        for l in self.intermediate: 
            out = l(out)
        
        class_logits = self.classifier(out)
        box_pred     = self.regressor(out) 

        if self.evaluate:
          softmax = torch.nn.Softmax(dim = 1)
          class_logits = softmax(class_logits)
            
        return class_logits, box_pred

def IOU(boxA, boxB):
    ##################################
    # TODO compute the IOU between the boxA, boxB boxes
    ##################################
    # This function computes the IOU between two set of boxes
    # Input: boxA :(n,4); boxB: (m,4)
    # Output: iou:(n,m)

    x_top_left = torch.max(boxA[:, 0].view(-1, 1), boxB[:, 0].view(1, -1))
    y_top_left = torch.max(boxA[:, 1].view(-1, 1), boxB[:, 1].view(1, -1))
    x_bottom_right = torch.min(boxA[:, 2].view(-1, 1), boxB[:, 2].view(1, -1))
    y_bottom_right = torch.min(boxA[:, 3].view(-1, 1), boxB[:, 3].view(1, -1))

    intersection_w = torch.max(torch.tensor([0.], device=device), x_bottom_right - x_top_left)
    intersection_h = torch.max(torch.tensor([0.], device=device), y_bottom_right - y_top_left)

    intersection_area = intersection_h * intersection_w

    union_area = ((boxA[:, 2] - boxA[:, 0]) * (boxA[:, 3] - boxA[:, 1])).view(-1, 1) \
                 + ((boxB[:, 2] - boxB[:, 0]) * (boxB[:, 3] - boxB[:, 1])).view(1, -1) - intersection_area

    iou = intersection_area / (union_area + 0.0001)

    return iou

def IOU3(boxA, boxB):
    ##################################
    # TODO compute the IOU between the boxA, boxB boxes
    ##################################
    # This function computes the IOU between two set of boxes
    # Input: boxA :(n,4); boxB: (m,4)
    # Output: iou:(n,m)

    x_top_left = torch.max(boxA[:, 0].view(-1, 1), boxB[:, 0].view(1, -1))
    y_top_left = torch.max(boxA[:, 1].view(-1, 1), boxB[:, 1].view(1, -1))
    x_bottom_right = torch.min(boxA[:, 2].view(-1, 1), boxB[:, 2].view(1, -1))
    y_bottom_right = torch.min(boxA[:, 3].view(-1, 1), boxB[:, 3].view(1, -1))

    intersection_w = torch.max(torch.tensor([0.]), x_bottom_right - x_top_left)
    intersection_h = torch.max(torch.tensor([0.]), y_bottom_right - y_top_left)

    intersection_area = intersection_h * intersection_w

    union_area = ((boxA[:, 2] - boxA[:, 0]) * (boxA[:, 3] - boxA[:, 1])).view(-1, 1) \
                 + ((boxB[:, 2] - boxB[:, 0]) * (boxB[:, 3] - boxB[:, 1])).view(1, -1) - intersection_area

    iou = intersection_area / (union_area + 0.0001)

    return iou

import numpy as np
import torch
from functools import partial
def MultiApply(func, *args, **kwargs):
    pfunc = partial(func, **kwargs) if kwargs else func
    map_results = map(pfunc, *args)
  
    return tuple(map(list, zip(*map_results)))

# This function compute the IOU between two set of boxes 
def IOU2( boxA, boxB):
        # both inputs are of the form: (x1,y1,x2,y2)

        xA = torch.where(boxA[:,:,0]> boxB[:,:,0], boxA[:,:,0], boxB[:,:,0])
        xB = torch.where(boxA[:,:,2]< boxB[:,:,2], boxA[:,:,2], boxB[:,:,2]) #min(boxA[:,:,2], boxB[:,:,2])
        yA = torch.where(boxA[:,:,1]> boxB[:,:,1], boxA[:,:,1], boxB[:,:,1]) #max(boxA[:,:,1], boxB[:,:,1])
        yB = torch.where(boxA[:,:,3]< boxB[:,:,3], boxA[:,:,3], boxB[:,:,3]) #min(boxA[:,:,3], boxB[:,:,3])
        zeroes = 0*xB
        interArea = torch.where(xB - xA > zeroes, xB - xA, zeroes)*torch.where(yB - yA > zeroes, yB - yA, zeroes)
        
        boxAarea = (boxA[:,:,2] - boxA[:,:,0])*(boxA[:,:,3] - boxA[:,:,1])

        boxBarea = (boxB[:,:,2] - boxB[:,:,0])*(boxB[:,:,3] - boxB[:,:,1])

        iou = interArea / (boxAarea + boxBarea - interArea)

# This function decodes the output of the box head that are given in the [t_x,t_y,t_w,t_h] format
# into box coordinates where it return the upper left and lower right corner of the bbox
# Input:
#       regressed_boxes_t: (total_proposals,4) ([t_x,t_y,t_w,t_h] format)
#       flatten_proposals: (total_proposals,4) ([x1,y1,x2,y2] format)
# Output:
#       box: (total_proposals,4) ([x1,y1,x2,y2] format)
# This function decodes the output of the box head that are given in the [t_x,t_y,t_w,t_h] format
# into box coordinates where it return the upper left and lower right corner of the bbox
# Input:
#       regressed_boxes_t: (total_proposals,4) ([t_x,t_y,t_w,t_h] format)
#       flatten_proposals: (total_proposals,4) ([x1,y1,x2,y2] format)
# Output:
#       box: (total_proposals,4) ([x1,y1,x2,y2] format)
def output_decoding(regressed_boxes_t,flatten_proposals, device='cpu'):
    x_p = (flatten_proposals[:,0] + flatten_proposals[:,2])/2
    y_p = (flatten_proposals[:,1] + flatten_proposals[:,3])/2
    w_p = flatten_proposals[:,2] - flatten_proposals[:,0]
    h_p = flatten_proposals[:,3] - flatten_proposals[:,1]

    t_star = torch.zeros_like(regressed_boxes_t)

    t_star[:,0] = regressed_boxes_t[:,0]*w_p + x_p
    t_star[:,1] = regressed_boxes_t[:,1]*h_p + y_p
    t_star[:,2] = torch.exp(regressed_boxes_t[:,2])*w_p
    t_star[:,3] = torch.exp(regressed_boxes_t[:,3])*h_p

    box = torch.zeros_like(t_star)
    box[:,0] = t_star[:,0] - t_star[:,2]/2
    box[:,1] = t_star[:,1] - t_star[:,3]/2
    box[:,2] = t_star[:,0] + t_star[:,2]/2
    box[:,3] = t_star[:,1] + t_star[:,3]/2

    return box

# loss_list = []
# loss_c_list = []
# loss_r_list = []

# checkpoint = torch.load('/content/drive/MyDrive/CIS680_2021/HW4_B/epoch28')
# loss = checkpoint['loss']
# loss_c=checkpoint['loss_class']
# loss_r= checkpoint['loss_regr']

# print(len(loss_c))

# t = 27426
# s = 979

# loss_list = [sum(loss[i:i+s])/s for i in range(0,t,s)] 
# loss_c_list = [sum(loss_c[i:i+s])/s for i in range(0,t,s)] 
# loss_r_list = [sum(loss_r[i:i+s])/s for i in range(0,t,s)] 

# plt.plot(loss_r_list[1:])
# plt.show()
# # print(loss)

# plt.figure(0)
# plt.plot(loss_list[1:])
# plt.xlabel('Epochs')
# plt.ylabel('Total Loss')
# # plt.savefig(save_path + 'total loss.png')
# plt.show()

# plt.figure(1)
# plt.plot(loss_c_list[1:])
# plt.xlabel('Epochs')
# plt.ylabel('Classifier Loss')
# # plt.savefig(save_path + 'loss_c.png')
# plt.show()

# plt.figure(2)
# plt.plot(loss_r_list[1:])
# plt.xlabel('Epochs')
# plt.ylabel('Regressor Loss')
# # plt.savefig(save_path + 'loss_r.png')
# plt.show()

"""### training code """

if __name__ == '__main__':
    # Put the path were you save the given pretrained model
    # pretrained_path='/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/Part B/checkpoint680.pth'
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    with torch.no_grad():
      backbone, rpn = pretrained_models_680(pretrained_path)
    
    boxHead = BoxHead(device=device)
    boxHead.to(device)
    # we will need the ImageList from torchvision
    from torchvision.models.detection.image_list import ImageList

    # imgs_path =   '/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/data/hw3_mycocodata_img_comp_zlib.h5'
    # masks_path =  '/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/data/hw3_mycocodata_mask_comp_zlib.h5'
    # labels_path = "/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/data/hw3_mycocodata_labels_comp_zlib.npy"
    # bboxes_path = "/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/data/hw3_mycocodata_bboxes_comp_zlib.npy"

    # Here we keep the top 20, but during training you should keep around 200 boxes from the 1000 proposals
    keep_topK= 200

    lr = 0.0007
    optimizer = torch.optim.Adam(boxHead.parameters(), lr=lr)
    num_epochs= 40
    epoch = -1
    
    resume = True
    model_path = "/content/drive/MyDrive/CIS680_2021/HW4_B/test/"

    loss_ = []
    loss_class_ = []
    loss_regr_  = []

    if resume == True:
      epoch_to_load = 'epoch27'
      path = model_path + str(epoch_to_load)
      checkpoint = torch.load(path)
      boxHead.load_state_dict(checkpoint['model_state_dict'])
      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
      epoch = checkpoint['epoch']
      loss_ = checkpoint['loss']
      loss_class_ = checkpoint['loss_class']
      loss_class_ = checkpoint['loss_regr']
    ground_dict = {}

    for epochs in range(epoch+1, num_epochs):
        if epochs == 7 or epochs == 12:
            for g in optimizer.param_groups:
              g['lr'] = g['lr']/10
        start = time.time()
        for iter, batch in enumerate(train_loader, 0):
            images = batch['images'].to(device)
            gt_label = batch['labels']
            bbox = batch['bbox']

            #with torch.no_grad():
            backout = backbone(images)
            # The RPN implementation takes as first argument the following image list
            im_lis = ImageList(images, [(800, 1088)]*images.shape[0])
            # Then we pass the image list and the backbone output through the rpn
            rpnout = rpn(im_lis, backout)

            #The final output is
            # A list of proposal tensors: list:len(bz){(keep_topK,4)}
            proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]
            # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}
            fpn_feat_list= list(backout.values())
          
            #Generating feature vectors 
            # import pdb;pdb.set_trace()
            feature_vectors = boxHead.MultiScaleRoiAlign(fpn_feat_list,proposals)
            labels, regressor_target = boxHead.create_ground_truth(proposals, gt_label, bbox)
            
            class_logits, box_pred   = boxHead.forward(feature_vectors)
            loss, loss_class, loss_regr = boxHead.compute_loss(class_logits, box_pred, labels.long(), regressor_target, effective_batch=32)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            loss_.append(loss)
            loss_class_.append(loss_class)
            loss_regr_.append(loss_regr)
            # np.load()
            if (iter+1)%10 == 0:
                print("epoch: %d , loss: %f, Regression Loss: %f, Classification Loss: %f" %(epochs, loss, loss_regr, loss_class))  
    
        if (epochs%1==0):
            print("loss: %d, Regression Loss: %f, Classification Loss: %f" %(loss, loss_class, loss_regr))
            print("Time per Epoch: ",  time.time()-start)
            print("Epoch %d/%d" %(epochs+1, num_epochs))
            np.save('/content/drive/MyDrive/CIS680_2021/HW4_B/test/loss_reg2',loss_regr_)
            np.save('/content/drive/MyDrive/CIS680_2021/HW4_B/test/loss_class2',loss_class_)
            np.save('/content/drive/MyDrive/CIS680_2021/HW4_B/test/loss_loss2',loss_)
            torch.save({
              'epoch': epochs,
              'model_state_dict': boxHead.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss' : loss_,
              'loss_class' : loss_class_,
              'loss_regr'  : loss_regr_ 
              }, model_path+"epoch"+str(epochs))

losst_ = []
losst_c_ = []
losst_r_ = []

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
backbone, rpn = pretrained_models_680(pretrained_path)
boxHead = BoxHead(device=device, evaluate = False)
boxHead.to(device)

backbone.eval()
rpn.eval()

s = 0

total_epochs = 20
for i in range(total_epochs):
  path = model_path+'epoch_lr_final_0.0007'+str(i)
  checkpoint = torch.load(path)
  boxHead.load_state_dict(checkpoint['model_state_dict'])
  boxHead.eval()

  with torch.no_grad():
      for iter, batch in enumerate(test_loader,0):
        print("Epoch: " + str(i) + "Iteration: " + str(iter))
        images = batch['images']
        gt_label = batch['labels']
        bbox = batch['bbox']
        backout = backbone(images)
        im_lis = ImageList(images, [(800, 1088)]*images.shape[0])
        rpnout = rpn(im_lis, backout)
        proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]
        fpn_feat_list= list(backout.values())
        feature_vectors = boxHead.MultiScaleRoiAlign(fpn_feat_list,proposals)
        labels, regressor_target = boxHead.create_ground_truth(proposals, gt_label, bbox)        
        class_logits, box_pred   = boxHead.forward(feature_vectors)
        loss, loss_c, loss_r = boxHead.compute_loss(class_logits, box_pred, labels.long(), regressor_target, effective_batch=32)

        losst_.append(loss.item())
        losst_c_.append(loss_c.item())
        losst_r_.append(loss_r.item())

  if i==0 :
    print(len(losst_))
    s= len(losst_)

"""## RUN"""

def nMS(clas,prebox,method='gauss',gauss_sigma=0.5):
    ##################################
    # TODO perform NMS
    ##################################
    # Input:
    #       clas: (top_k_boxes) (scores of the top k boxes)
    #       prebox: (top_k_boxes,4) (coordinate of the top k boxes)
    # Output:
    #       nms_clas: (Post_NMS_boxes)
    #       nms_prebox: (Post_NMS_boxes,4)

    ious = IOU(prebox,prebox).triu(diagonal=1)

    ious_cmax, ids = torch.max(ious, dim=0)
    ious_cmax = ious_cmax.expand(ious_cmax.shape[0], ious_cmax.shape[0]).T

    if method == 'gauss':
        decay = torch.exp(-(ious ** 2 - ious_cmax ** 2) / gauss_sigma)
    else:
        decay = (1 - ious) / (1 - ious_cmax)

    decay, ids = torch.min(decay, dim=0)

    nms_clas = clas*decay

    return nms_clas

def postprocess_detections1(class_logits, box_regression, proposals, conf_thresh=0.5, keep_num_preNMS=500, keep_num_postNMS=50):
    # This function does the post processing for the results of the Box Head for a batch of images
    # Use the proposals to distinguish the outputs from each image
    # Input:
    #       class_logits: (total_proposals,(C+1))
    #       box_regression: (total_proposal,4*C)           ([t_x,t_y,t_w,t_h] format)
    #       proposals: list:len(bz)(per_image_proposals,4) (the proposals are produced from RPN [x1,y1,x2,y2] format)
    #       conf_thresh: scalar
    #       keep_num_preNMS: scalar (number of boxes to keep pre NMS)
    #       keep_num_postNMS: scalar (number of boxes to keep post NMS)
    # Output:
    #       boxes: list:len(bz){(post_NMS_boxes_per_image,4)}  ([x1,y1,x2,y2] format)
    #       scores: list:len(bz){(post_NMS_boxes_per_image)}   ( the score for the top class for the regressed box)
    #       labels: list:len(bz){(post_NMS_boxes_per_image)}   (top class of each regressed box)

    pred_score, pred_class = torch.max(class_logits, dim=1)

    pred_box=box_regression[pred_class!=0]
    pred_score = pred_score[pred_class!=0]
    total_proposals = torch.cat(proposals)
    total_proposals = total_proposals[pred_class != 0]
    pred_class = pred_class[pred_class!=0]

    col_index = torch.stack([4*pred_class-4, 4*pred_class-3, 4*pred_class-2, 4*pred_class-1]).T
    pred_box = pred_box[torch.arange(pred_class.shape[0]).view(-1,1),col_index]

    decoded_boxes = output_decoding(pred_box, total_proposals)

    pre_NMS_boxes = decoded_boxes[pred_score>conf_thresh]
    pre_NMS_class = pred_class[pred_score>conf_thresh]
    pre_NMS_score = pred_score[pred_score>conf_thresh]


    indices = torch.argsort(pre_NMS_score, descending=True)
    top_scores = pre_NMS_score[indices[:keep_num_preNMS]]
    top_class = pre_NMS_class[indices[:keep_num_preNMS]]
    top_boxes = pre_NMS_boxes[indices[:keep_num_preNMS]]

    top_boxes[:,[0,2]]=torch.clamp(top_boxes[:,[0,2]], min=0, max=1087)
    top_boxes[:,[1,3]]=torch.clamp(top_boxes[:,[1,3]], min=0, max=799)

    # if NMS==False:
    #     return top_boxes, top_scores, top_class
    
    # print("top_scores[top_class==1]: ", top_scores[top_class==1])
    # print("top_boxes[top_class==1]: ", top_boxes[top_class==1])
    # print("len: ", len(top_scores[top_class==1]))
    if len(top_scores[top_class==1])>0:
        decay_scores_1 = nMS(top_scores[top_class==1],top_boxes[top_class==1])
    else:
        decay_scores_1 = torch.tensor([], device=device)

    if len(top_scores[top_class==2])>0:
        decay_scores_2 = nMS(top_scores[top_class==2],top_boxes[top_class==2])
    else:
        decay_scores_2 = torch.tensor([], device=device)

    if len(top_scores[top_class==3])>0:
        decay_scores_3 = nMS(top_scores[top_class==3],top_boxes[top_class==3])
    else:
        decay_scores_3 = torch.tensor([], device=device)

    post_NMS_boxes = torch.cat((top_boxes[top_class==1], top_boxes[top_class==2], top_boxes[top_class==3]))
    post_NMS_scores = torch.cat((decay_scores_1,decay_scores_2,decay_scores_3))
    post_NMS_class = torch.cat((top_class[top_class==1], top_class[top_class==2], top_class[top_class==3]))

    indices = torch.argsort(post_NMS_scores, descending=True)
    post_NMS_scores = post_NMS_scores[indices[:keep_num_postNMS]]
    post_NMS_boxes = post_NMS_boxes[indices[:keep_num_postNMS]]
    post_NMS_class = post_NMS_class[indices[:keep_num_postNMS]]

    return post_NMS_boxes, post_NMS_scores, post_NMS_class

"""# **Our working Post processing is here**"""

def postprocess_detections(class_logits, box_regression, proposals, conf_thresh=0.5, keep_num_preNMS=500, keep_num_postNMS=50):
    #Can take only single image data
  
        # class_logits = class_logits.cpu()
        # box_regression = box_pred.cpu()
        
        conf, clas = torch.max(class_logits, dim=1)

        #Removing the background and predictions less than conf_thresh
        # print(clas.shape)
        # print("examp_clas: ", examp[clas!=0])
        id = torch.nonzero(clas, as_tuple=False).flatten()
        clas = clas[id]
        conf = conf[id]
        boxes = box_regression[id]
        proposals = torch.cat(proposals)
        proposals = proposals[id]
        

        # boxes = box_regression[id][conf > conf_thresh]
        # proposals = proposals[id][conf > conf_thresh]
        # conf = conf[conf > conf_thresh]

        # print(clas)
        # examp = clas
        # id = conf[clas] > conf_thresh
        # clas = clas[conf[clas] > conf_thresh]
        # # print("conf[examp!=0]: ", conf[examp!=0])
        # print("box_regression[conf[clas] > conf_thresh]: ", box_regression[id])

        # conf = conf[clas]
        # boxes = box_regression[clas]
        # print("boxes: ", boxes)
        
        

        #Extracting the boxes corresponding to predicted labels
        box_col_ind = torch.stack([4*clas-4, 4*clas-3, 4*clas-2, 4*clas-1]).T
        box_row_ind = torch.arange(clas.shape[0]).view(-1,1)
        boxes = boxes[box_row_ind, box_col_ind]

        boxes = output_decoding(boxes, proposals)

        boxes = boxes[conf > conf_thresh]
        clas = clas[conf > conf_thresh]
        conf = conf[conf > conf_thresh]
        



        #Keeping top preds pre NMS
        sort_ind = torch.argsort(conf, descending=True)
        conf = conf[sort_ind[:keep_num_preNMS]]
        clas = clas[sort_ind[:keep_num_preNMS]]
        boxes = boxes[sort_ind[:keep_num_preNMS]]

        #Cropping the boundaries
        boxes[:,[0,2]] = torch.clamp(boxes[:,[0,2]], min=0, max=1087)
        boxes[:,[1,3]] = torch.clamp(boxes[:,[1,3]], min=0, max=799)

        #Applying NMS Independently for each class
        if len(conf[clas==1])>0:
          conf_1 = MatrixNMS(boxes[clas==1], conf[clas==1])
        else:
          conf_1 = torch.tensor([]).to(device)
        
        if len(conf[clas==2])>0:
          conf_2 = MatrixNMS(boxes[clas==2], conf[clas==2])
        else:
          conf_2 = torch.tensor([]).to(device)
        
        if len(conf[clas==3])>0:
          conf_3 = MatrixNMS(boxes[clas==3], conf[clas==3])
        else:
          conf_3 = torch.tensor([]).to(device)
        
        #Keeping N preds post NMS
        NMS_conf = torch.cat((conf_1, conf_2, conf_3))
        sort_NMS_ind = torch.argsort(NMS_conf, descending=True)
        NMS_conf = NMS_conf[sort_NMS_ind[:keep_num_postNMS]]
        NMS_boxes = torch.cat((boxes[clas==1], boxes[clas==2], boxes[clas==3]))[sort_NMS_ind[:keep_num_postNMS]]
        NMS_class = torch.cat((clas[clas==1], clas[clas==2], clas[clas==3]))[sort_NMS_ind[:keep_num_postNMS]]


        return NMS_boxes, NMS_conf, NMS_class

def MatrixNMS(sorted_boxes, sorted_scores, method='gauss', gauss_sigma=0.5):
        # n = len(sorted_scores)
        # sorted_masks = sorted_masks.reshape(n, -1)
        # intersection = torch.mm(sorted_masks, sorted_masks.T)
        # areas = sorted_masks.sum(dim=1).expand(n, n)
        # union = areas + areas.T - intersection
        # ious = (intersection / union).triu(diagonal=1)

        ious = IOU(sorted_boxes, sorted_boxes).triu(diagonal=1)
        ious_cmax = ious.max(0)[0]
        ious_cmax = ious_cmax.expand(ious_cmax.shape[0], ious_cmax.shape[0]).T
        if method == 'gauss':
            decay = torch.exp(-(ious ** 2 - ious_cmax ** 2) / gauss_sigma)
        else:
            decay = (1 - ious) / (1 - ious_cmax)
        decay = decay.min(dim=0)[0]
        return sorted_scores * decay

"""### Average Precision"""

def for_AP(conf, pred_class, pred_boxes, gt_class, gt_boxes):
    #can take only single image data
        # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        matches = {1: [], 2: [], 3: []}
        confs = {1: [], 2: [], 3: []}
        trues = {1: 0, 2: 0, 3: 0}

        #for each class
        conf =conf.cpu()
        for i in range(1,4):
          gt_ind = gt_class == i
          pred_ind = pred_class == i
          gt_per_class = gt_class[gt_ind]
          pred_per_class = pred_class[pred_ind]

          if pred_per_class.shape[0] > 0:
            predbox_per_class = pred_boxes[pred_class == i] 

            if gt_per_class.numel() > 0:
            #   print(gt_ind)  
            #   print('gt_box',gt_boxes)
            #   print('gt_class',gt_class)
              gtbox_per_class = gt_boxes[gt_class == i]
              

              iou = IOU3(predbox_per_class, gtbox_per_class)
              match = torch.sum(iou > 0.5, dim = 1).clamp(min=0,max=1).cpu().tolist()

            else:
              match = [0]*pred_per_class.shape[0]
            
            matches[i].extend(match)
            confs[i].extend(conf[pred_ind])

          if gt_per_class.numel() > 0:
            trues[i] += gt_per_class.shape[0]
          
          else:
            trues[i] += 0
          
        return matches, confs, trues

def average_precision_with_curve( match, confs, gt_trues):
    AP = {}
    for j in range(1,4):
      if len(matches[j]) > 0:
        conf = np.array(confs[j])
        match = np.array(matches[j])
        conf_max = max(conf)
        precision = np.zeros((100))
        recall = np.zeros((100))
        length = np.linspace(0.5, conf_max, 100)

        for i, value in enumerate(length):
          pred_positives = match[conf > value].shape[0]
          TP = sum(match[conf > value])
          p = 1
          if pred_positives > 0:
            p = TP/pred_positives
          
          r = 1
          if gt_trues > 0:
            r = TP/gt_trues
          precision[i] = p
          recall[i] = r

        plt.figure(0)
        plt.xlim([0,1])
        plt.ylim([0,1])
        if j == 1:
          plt.plot(recall, precision, label = "Vehicle")
        if j == 2:
          plt.plot(recall, precision, label = "Person")
        if j == 3:
          plt.plot(recall, precision, label = "Animal")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.legend()
        
        ind=np.argsort(recall)
        recall=recall[ind]
        precision=precision_mat[ind]

        AP[j] = metrics.auc(recall, precision)
        print("Average Precision for the Class ", j, " is ", AP[j])
    
    return APdrive.mount("/content/drive", force_remount=True)


def average_precision(matches, confs, gt_trues):
    AP = {}
    # print('confs',confs)
    for j in range(1,4):
      if len(matches[j]) > 0:
        # print('conf_j',confs[j])
        conf = np.array(confs[j])
        match = np.array(matches[j])
        conf_max = np.amax(conf)
        conf_max = conf_max.detach().cpu()
        # print('conf_max',conf_max)
        # print(conf_max.type())
        # print('conf',conf)
        precision = np.zeros((100))
        recall = np.zeros((100))
        length = np.linspace(0.5, conf_max, 100)

        for i, value in enumerate(length):
          pred_positives = match[conf > value].shape[0]
          TP = sum(match[conf > value])
          p = 0
          if pred_positives > 0:
            p = TP/pred_positives
          
          r = 0
          if gt_trues[j] > 0:
            r = TP/gt_trues[j]
          precision[i] = p
          recall[i] = r

        plt.figure(0)
        plt.xlim([0,1])
        plt.ylim([0,1])
        if j == 1:
          plt.plot(recall, precision, label = "Vehicle")
        if j == 2:
          plt.plot(recall, precision, label = "Person")
        if j == 3:
          plt.plot(recall, precision, label = "Animal")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.legend()

        ind=np.argsort(recall)
        recall=recall[ind]
        precision=precision[ind]

        AP[j] = auc(recall, precision)

        print("Average Precision for the Class ", j, " is ", AP[j])
    
    return AP

"""## mAP"""



def meanAP(AP):
    l = 0
    m = 0
    for key in AP:
      m += AP[key]
      l = key
    m = m/l
    
    return m

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
boxHead = BoxHead()
# mode.eval()
boxHead = boxHead.to(device)
checkpoint = torch.load('/content/drive/MyDrive/CIS680_2021/HW_4/test/epoch35')
boxHead.load_state_dict(checkpoint['model_state_dict'])
color_list = ['red','blue', 'green']

with torch.no_grad():
      backbone, rpn = pretrained_models_680(pretrained_path)

boxHead.evaluate = True
boxHead.eval()
backbone.eval()
rpn.eval()
start = time.time()
keep_topK = 200
keep_num_preNMS = 20
invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],
                                              std = [ 1/0.229, 1/0.224, 1/0.225 ]),
                        transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],
                                              std = [ 1., 1., 1. ]),
                        ])

# save_path = "/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/Part B/7.3/"
ind = 0
conf_thresh = 0.5
for iter, batch in enumerate(test_loader, 0):
    images = batch['images']
    gt_label = batch['labels']
    bbox = batch['bbox']

    #with torch.no_grad():
    backout = backbone(images)
    # The RPN implementation takes as first argument the following image list
    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])
    # Then we pass the image list and the backbone output through the rpn
    rpnout = rpn(im_lis, backout)

    #The final output is
    # A list of proposal tensors: list:len(bz){(keep_topK,4)}
    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]
    # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}
    fpn_feat_list= list(backout.values())
  
    #Generating feature vectors 
    # import pdb;pdb.set_trace()
    feature_vectors = boxHead.MultiScaleRoiAlign(fpn_feat_list,proposals)
    labels, regressor_target = boxHead.create_ground_truth(proposals, gt_label, bbox)

    prop = proposals[0]
    print(prop[0])
    print("wp: ", wp.shape)

    wp = prop[:20,2] - prop[:20,0]
    hp = prop[:20,3] - prop[:20,1]
    xp = (prop[:20,0] + prop[:20,2])/2
    yp = (prop[:20,1] + prop[:20,3])/2
    
    gt_box= torch.zeros(regressor_target[:20,:].shape)
    gt_box[:,0] = regressor_target[:20,0]*wp + xp - torch.exp(regressor_target[:20,2])*wp/2
    gt_box[:,1] = regressor_target[:20,1]*hp + yp - torch.exp(regressor_target[:20,3])*hp/2
    gt_box[:,2] = regressor_target[:20,0]*wp + xp + torch.exp(regressor_target[:20,2])*wp/2 
    gt_box[:,3] = regressor_target[:20,1]*hp + yp + torch.exp(regressor_target[:20,3])*hp/2

    images = invTrans(images)
    plt.figure(1)
    plt.imshow(images[0,:,:,:].cpu().numpy().transpose(1,2,0))
    
    for i in range(prop[:20].shape[0]):
      if labels[i] == 1:
        coord = gt_box[i,:]
        plt.plot([coord[0], coord[2], coord[2], coord[0], coord[0]], [coord[1], coord[1], coord[3], coord[3], coord[1]], 'c-')
        coord = prop[i,:]
        plt.plot([coord[0], coord[2], coord[2], coord[0], coord[0]], [coord[1], coord[1], coord[3], coord[3], coord[1]], 'r-')
    # plt.savefig(save_path + str(ind) + 'vehicle')

    plt.show()

    plt.figure(2)
    plt.imshow(images[0,:,:,:].cpu().numpy().transpose(1,2,0))
    for i in range(prop[:20].shape[0]):
      if labels[i] == 2:
        coord = gt_box[i,:]
        plt.plot([coord[0], coord[2], coord[2], coord[0], coord[0]], [coord[1], coord[1], coord[3], coord[3], coord[1]], 'c-')
        coord = prop[i,:]
        plt.plot([coord[0], coord[2], coord[2], coord[0], coord[0]], [coord[1], coord[1], coord[3], coord[3], coord[1]], 'g-')
    # plt.savefig(save_path + str(ind) + 'person')
    plt.show()
    
    plt.figure(3)
    plt.imshow(images[0,:,:,:].cpu().numpy().transpose(1,2,0))
    for i in range(prop[:20].shape[0]):
      if labels[i] == 3:
        coord = gt_box[i,:]
        plt.plot([coord[0], coord[2], coord[2], coord[0], coord[0]], [coord[1], coord[1], coord[3], coord[3], coord[1]], 'c-')
        coord = prop[i,:]
        plt.plot([coord[0], coord[2], coord[2], coord[0], coord[0]], [coord[1], coord[1], coord[3], coord[3], coord[1]], 'b-')
    # plt.savefig(save_path + str(ind) + 'animal')
    ind +=1
    plt.show()
    if(iter == 10):
      break    







    
    # class_logits, box_pred   = boxHead.forward(feature_vectors)

    # l = 20
    # conf_thresh = 0.5
    # keep_num_preNMS = 50
    # keep_num_post_NMS = 3
    # NMS = True

    # # boxes_to_plot, scores, labels_to_plot = boxHead.postprocess_detections(class_logits, box_pred, proposals, conf_thresh, keep_num_preNMS, keep_num_post_NMS)
    # boxes_to_plot, scores, labels_to_plot = postprocess_detections(class_logits, box_pred, proposals, conf_thresh, keep_num_preNMS, keep_num_post_NMS)
    # # boxes_to_plot, scores, labels_to_plot = postprocess_detections1(class_logits, box_pred, proposals, conf_thresh, keep_num_preNMS, keep_num_post_NMS)
    # images = transforms.functional.normalize(images[0],
    #                                                 [-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],
    #                                                 [1 / 0.229, 1 / 0.224, 1 / 0.225], inplace=False)
    
    # images = images.cpu().float()
    # plt.figure(iter)
    # ax1 = plt.gca()
    # plt.imshow(images.permute(1, 2, 0))
    # title = 'Pre NMS Top ' + str(keep_num_preNMS)
    # if NMS:
    #     title = 'Post NMS Top ' + str(keep_num_post_NMS)

    # plt.title(title)

    # # print("boxes_to_plot: ",boxes_to_plot)
    # for elem in range(boxes_to_plot.shape[0]):
    #     coord = boxes_to_plot[elem, :].view(-1)

    #     # print("label: ", labels_to_plot[elem])
    #     col = color_list[labels_to_plot[elem]-1]
    #     rect = patches.Rectangle((coord[0], coord[1]), coord[2] - coord[0], coord[3] - coord[1], fill=False,
    #                                 color=col)
    #     ax1.add_patch(rect)
    
    # print("boxes_to_plot: ",boxes_to_plot)
    # for elem, value in enumerate(boxes_to_plot[0]):
    #   # coord = boxes_to_plot[elem].view(-1)
    #   coord = value.view(-1)
    #   print("label: ", labels_to_plot[0][elem])

    #   col = color_list[labels_to_plot[0][elem]-1]
    #   rect = patches.Rectangle((coord[0], coord[1]), coord[2] - coord[0], coord[3] - coord[1], fill=False,
    #                               color=col)
    #   ax1.add_patch(rect)





    # class_logits = class_logits.cpu()
    # box_regression = box_pred.cpu()
  
    # class_scores, class_idx = torch.max(class_logits, dim =1)

    # background = class_idx==0
    # class_scores[background] = 0

    # class_idx_cp = class_idx.clone() - 1
    # class_idx_cp[class_idx == 0] = 0

    # cols_to_idx           =     np.linspace(4*class_idx_cp,4*class_idx_cp+3,4).T
    # rows                  =     np.arange(box_regression.shape[0]).reshape(-1,1)

    # boxes_regr        = box_regression[rows, cols_to_idx]

    # boxes  = []
    # scores = []
    # labels = []
    
    if iter == 30:
      break



device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
boxHead = BoxHead()
# mode.eval()
boxHead = boxHead.to(device)
checkpoint = torch.load('/content/drive/MyDrive/CIS680_2021/HW_4/test/epoch35')
boxHead.load_state_dict(checkpoint['model_state_dict'])
color_list = ['red','blue', 'green']
matches = {1: [], 2: [], 3: []}
score = {1: [], 2: [], 3: []}
trues = {1: 0, 2: 0, 3: 0}
with torch.no_grad():
      backbone, rpn = pretrained_models_680(pretrained_path)

boxHead.evaluate = True
boxHead.eval()
backbone.eval()
rpn.eval()
start = time.time()
keep_topK = 200
keep_num_preNMS = 20
invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],
                                              std = [ 1/0.229, 1/0.224, 1/0.225 ]),
                        transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],
                                              std = [ 1., 1., 1. ]),
                        ])

# save_path = "/content/drive/My Drive/Upenn Courses/2. Fall 2020/CIS 680: Advanced Topics in Machine Perception/Project 4: Mask RCNN/Part B/7.3/"
ind = 0
conf_thresh = 0.5
cnt = 0
for iter, batch in enumerate(test_loader, 0):
    images = batch['images']
    gt_label = batch['labels']
    bbox = batch['bbox']
    del batch

    #with torch.no_grad():
    backout = backbone(images)
    # The RPN implementation takes as first argument the following image list
    im_lis = ImageList(images, [(800, 1088)]*images.shape[0])
    # Then we pass the image list and the backbone output through the rpn
    rpnout = rpn(im_lis, backout)

    #The final output is
    # A list of proposal tensors: list:len(bz){(keep_topK,4)}
    proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]
   

    # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}
    fpn_feat_list= list(backout.values())
    del backout, rpnout, im_lis
    torch.cuda.empty_cache()
  
    #Generating feature vectors 
    # import pdb;pdb.set_trace()
    feature_vectors = boxHead.MultiScaleRoiAlign(fpn_feat_list,proposals)
    labels, regressor_target = boxHead.create_ground_truth(proposals, gt_label, bbox)
    del fpn_feat_list
    torch.cuda.empty_cache()
    
    class_logits, box_pred   = boxHead.forward(feature_vectors)

    l = 20
    conf_thresh = 0.5
    keep_num_preNMS = 200
    keep_num_post_NMS = 20
    NMS = True

    boxes_to_plot, scores, labels_to_plot = postprocess_detections(class_logits, box_pred, proposals, conf_thresh, keep_num_preNMS, keep_num_post_NMS)
    match_img, scores_img, trues_img = for_AP(scores.cpu(), labels_to_plot.cpu(), boxes_to_plot.cpu(), gt_label[0].cpu(), bbox[0].cpu())
    # images = transforms.functional.normalize(images[0],
    #                                                 [-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],
    #                                                 [1 / 0.229, 1 / 0.224, 1 / 0.225], inplace=False)
    # images = images.cpu().float()
    # plt.figure(iter)
    # ax1 = plt.gca()
    # plt.imshow(images.permute(1, 2, 0))
    # title = 'Pre NMS Top ' + str(keep_num_preNMS)
    # if NMS:
    #     title = 'Post NMS Top ' + str(keep_num_post_NMS)

    # plt.title(title)

    # for elem in range(boxes_to_plot.shape[0]):
    #     coord = boxes_to_plot[elem, :].view(-1)

    #     col = color_list[labels_to_plot[elem]-1]
    #     rect = patches.Rectangle((coord[0], coord[1]), coord[2] - coord[0], coord[3] - coord[1], fill=False,
    #                                 color=col)
    #     ax1.add_patch(rect)
  
    # #Modify the for loop in line 127 in main_test.py as below considering match_img, scores_img, and trues_img are the output of forAP()
    for i in range(1,4):
        matches[i].extend(match_img[i])
        score[i].extend(scores_img[i])
        trues[i] += trues_img[i]
    
    # del match_img,scores_img,trues_img

    del feature_vectors
    torch.cuda.empty_cache()

    del proposals, gt_label, bbox, match_img, scores_img, trues_img
    torch.cuda.empty_cache()

    del class_logits, box_pred, images, boxes_to_plot, scores, labels_to_plot
    # del fpn_feat_list,cate_pred_list,ins_pred_list,ins_gts_list,\
    # ins_ind_gts_list,cate_gts_list,cate_loss,mask_loss,total_loss
    torch.cuda.empty_cache()
    cnt += 1
    print(cnt)
    
    # class_logits = class_logits.cpu()
    # box_regression = box_pred.cpu()
  
    # class_scores, class_idx = torch.max(class_logits, dim =1)

    # background = class_idx==0
    # class_scores[background] = 0

    # class_idx_cp = class_idx.clone() - 1
    # class_idx_cp[class_idx == 0] = 0

    # cols_to_idx           =     np.linspace(4*class_idx_cp,4*class_idx_cp+3,4).T
    # rows                  =     np.arange(box_regression.shape[0]).reshape(-1,1)

    # boxes_regr        = box_regression[rows, cols_to_idx]

    # boxes  = []
    # scores = []
    # labels = []
    
    # if iter == 10:
    #   break

AP = average_precision(matches, score, trues)
MAP = meanAP(AP)
print("Average Precision for each class: ", AP)
print("Mean Average Precision: ", MAP)

####AP code 
def average_precision(match_values, score_values, total_trues):

    # print(match_values.detach().numpy())
    match_values = np.array(match_values)
    score_values = np.array(score_values)
    print(match_values)
    max_score = max(score_values).detach().cpu()
    ln = np.linspace(0.5, max_score, 100)
    precision_values = np.zeros((100))
    recall_values = np.zeros((100))

    for i, thresh in enumerate(ln):
        matches = match_values[score_values > thresh]
        true_positive = sum(matches)
        total_positive = matches.shape[0]

        precision = 1
        if total_positive > 0:
            precision = true_positive / total_positive

        recall = 1
        if total_trues > 0:
            recall = true_positive / total_trues

        precision_values[i] = precision
        recall_values[i] = recall

    pass

    sorted_indices = np.argsort(recall_values)
    sorted_recall = recall_values[sorted_indices]
    sorted_precision = precision_values[sorted_indices]

    area = auc(sorted_recall, sorted_precision)

    return area

AP = 0
count = 0
mAP = 0

for k in range(1,4):
  if len(matches[k]) > 0:
    area = average_precision(matches[k], score[k], trues[k])
    print("Average precision for class ", str(k+1), area)
    AP += area
    count += 1

if count >0:
    mAP = AP/count

####AP code 
def average_precision(match_values, score_values, total_trues):
    area = 0
    AP = {}
    for j in range(1,4):
      if len(match_values[j]) > 0:
        # maximum_score = score_values[j].max()
        maximum_score = max(score_values[j]).detach().cpu()
        ln= np.linspace(0.6,maximum_score,num=100)
        precision_mat= np.zeros(101)
        recall_mat= np.zeros(101)
        for i ,th in enumerate(ln):
          # th = 0.2
          # match_values[j]         = [ 1   0  1   0  0 0 0 0  1  1  1  1]
          # score_values[j]         = [.5  .5 .3  .9  0 0 0 0 .1 .2 .8 .3]
          # score_values[j] > th    = [ T   T  T   T  F F F F  F  F  T  T]
          # matches                 = [ 1   0  1   0                 1  1]
          # total_positive should be 6
          # TP should be 4
          matches = np.array(match_values[j])[score_values[j] >= th]
          TP = matches.sum()
          total_positive = matches.shape[0]
          # total_positive = sum(match_values[j])
          
          precision = 1
          if total_positive > 0:
            precision = TP/total_positive
          recall =1
          if total_trues[j] > 0:
            recall = TP/total_trues[j]
          precision_mat[i] = precision
          recall_mat[i] = recall
        # precision_mat[100] = 1
        # recall_mat[100] = 0
        plt.figure(0)
        plt.ylim([0,1])
        plt.xlim([0,1])
        
        if j == 1:
          plt.plot(recall_mat,precision_mat, 'r-', label = "Vehicle")
        if j == 2:
          plt.plot(recall_mat,precision_mat, 'g-', label = "Person")
        if j == 3:
          plt.plot(recall_mat,precision_mat, 'b-', label = "Animal")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.legend()
        #plt.title("Class: "+ str(j))
        #plt.savefig("Class: " + str(j) + " curve")
        sorted_ind=np.argsort(recall_mat)
        sorted_recall=recall_mat[sorted_ind]
        sorted_precision=precision_mat[sorted_ind]

        AP[j] = auc(sorted_recall,sorted_precision)
        # area += auc(sorted_recall,sorted_precision)
    return AP

AP = average_precision(matches, score, trues)
MAP = meanAP(AP)
print("Average Precision for each class: ", AP)
print("Mean Average Precision: ", MAP)

import torchvision
import torch
import numpy as np
# from BoxHead import *
# from utils import *
# from pretrained_models import *

if __name__ == '__main__':

    # Put the path were you save the given pretrained model
    pretrained_path='/content/drive/MyDrive/CIS680_2021/HW4_B/checkpoint680.pth'
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    backbone, rpn = pretrained_models_680(pretrained_path)

    # we will need the ImageList from torchvision
    from torchvision.models.detection.image_list import ImageList

    # Put the path were the given hold_out_images.npz file is save and load the images
    hold_images_path='/content/drive/MyDrive/CIS680_2021/HW4_B/hold_out_images.npz'
    test_images=np.load(hold_images_path,allow_pickle=True)['input_images']


    # Load your model here. If you use different parameters for the initialization you can change the following code
    # accordingly
    boxHead=BoxHead(device=device)
    boxHead=boxHead.to(device)
    boxHead.eval()

    # Put the path were you have your save network
    train_model_path='/content/drive/MyDrive/CIS680_2021/HW4_B/epoch28'
    checkpoint = torch.load(train_model_path)
    # reload models
    boxHead.load_state_dict(checkpoint['model_state_dict'])
    keep_topK=200

    cpu_boxes = []
    cpu_scores = []
    cpu_labels = []

    for i, numpy_image in enumerate(test_images, 0):
        images = torch.from_numpy(numpy_image).to(device)
        with torch.no_grad():
            # Take the features from the backbone
            backout = backbone(images)

            # The RPN implementation takes as first argument the following image list
            im_lis = ImageList(images, [(800, 1088)]*images.shape[0])
            # Then we pass the image list and the backbone output through the rpn
            rpnout = rpn(im_lis, backout)

            #The final output is
            # A list of proposal tensors: list:len(bz){(keep_topK,4)}
            proposals=[proposal[0:keep_topK,:] for proposal in rpnout[0]]
            # A list of features produces by the backbone's FPN levels: list:len(FPN){(bz,256,H_feat,W_feat)}
            fpn_feat_list= list(backout.values())


            feature_vectors=boxHead.MultiScaleRoiAlign(fpn_feat_list,proposals)

            class_logits,box_pred=boxHead(feature_vectors)

            # Do whaterver post processing you find performs best
            boxes,scores,labels=boxHead.postprocess_detections(class_logits,box_pred,proposals,conf_thresh=0.8, keep_num_preNMS=200, keep_num_postNMS=3)

            for box, score, label in zip(boxes,scores,labels):
                if box is None:
                    cpu_boxes.append(None)
                    cpu_scores.append(None)
                    cpu_labels.append(None)
                else:
                    cpu_boxes.append(box.to('cpu').detach().numpy())
                    cpu_scores.append(score.to('cpu').detach().numpy())
                    cpu_labels.append(label.to('cpu').detach().numpy())

    np.savez('predictions.npz', predictions={'boxes': cpu_boxes, 'scores': cpu_scores,'labels': cpu_labels})
